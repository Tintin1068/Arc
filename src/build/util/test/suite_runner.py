# Copyright 2014 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Defines the integration test interface to running a suite of tests."""

import filtered_subprocess
import fnmatch
import json
import logging
import os
import subprocess
import threading

import build_common
from util import file_util
from util import launch_chrome_util
from util import logging_util
from util.test import scoreboard
from util.test import suite_runner_config
from util.test import suite_runner_config_flags as flags
from util.test import suite_runner_util
from util.test import test_method_result


# Number of times to retry because of Chrome startup flake.
LAUNCH_CHROME_FLAKE_RETRY_COUNT = 3
# This is the minimum line count that is required to consider Chrome to have
# launched correctly.
_LAUNCH_CHROME_MINIMUM_LINES = 16
# CRX directories used by system mode should have this prefix.
SYSTEM_MODE_PREFIX = 'system_mode.'


class TimeoutError(Exception):
  """Timeout class raised in this module."""


class SuiteRunnerOutputHandler(object):
  """Class to handle output generated by test runners.

  When a test suite runner is run, it will use a filtered_subprocess
  to execute the tests, using this class as an output handler.  This
  class will write all output from the test run to disk and then allow
  the runner to perform further processing.
  """
  def __init__(self, verbose, output_file, runner):
    self._output_file = output_file
    self._verbose = verbose
    self._runner = runner
    self._output = []

  def is_done(self):
    return False

  def handle_stdout(self, txt):
    self._handle_output(txt)

  def handle_stderr(self, txt):
    self._handle_output(txt)

  def get_output(self):
    return ''.join(self._output)

  def _handle_output(self, txt):
    if txt:
      if self._verbose:
        print '%s %s' % (self._runner.name, txt.strip())
      self._output_file.write(txt)
      self._output.append(txt)
      self._runner.handle_output(txt)


class SuiteRunnerBase(object):
  """Base class for a test suite runner.

  A suite runner can be constructed with several keyword options:

    flags

      A combination of util.test.suite_runner_config_flags flag values that
      indicate the expected result of running the suite.

    suite_test_expectations

      A dictionary mapping fully qualified test names to an expectation flag
      value indicating if that test will pass or not.

      Example:

        suite_test_expectations = {
            'Class1': {
                'method1': PASS, # passes
                'method2': FLAKY,  # passes, but flaky
                'method3': FAIL,  # fails
                'method4': NOT_SUPPORTED  # blacklist from running
            }
        }

    deadline

        The deadline in seconds in which the test suite should run to
        completion.
    """
  DEFAULT_DEADLINE = 300

  WRITE_ARGS_MIN_LENGTH = 10000

  # Output from tests will be written in this directory.
  _output_directory = 'out/integration_tests'

  @classmethod
  def set_output_directory(cls, output_directory):
    cls._output_directory = output_directory

  @classmethod
  def get_output_directory(cls):
    return cls._output_directory

  @staticmethod
  def get_xvfb_args(output_filename):
    assert output_filename, 'output_filename is not specified.'
    return ['xvfb-run', '--auto-servernum',
            # Use 24-bit color depth, as Chrome does not work with 8-bit color
            # depth, which is used in xvfb-run by default.
            '--server-args', '-screen 0 640x480x24',
            '--error-file', output_filename]

  def __init__(self, name, base_expectation_map, config=None):
    merged_config = suite_runner_config.default_run_configuration()
    if config:
      merged_config.update(config)

    self._lock = threading.Lock()
    self._name = name
    self._terminated = False
    self._deadline = merged_config.pop('deadline')
    self._bug = merged_config.pop('bug')
    self._metadata = merged_config.pop('metadata')
    self._test_order = merged_config.pop('test_order')

    default_expectation = merged_config.pop('flags')
    override_expectation_map = merged_config.pop('suite_test_expectations')
    assert not merged_config, ('Unexpected keyword arguments %s' %
                               merged_config.keys())

    expectation_map = suite_runner_util.merge_expectation_map(
        base_expectation_map, override_expectation_map, default_expectation)
    self._scoreboard = scoreboard.Scoreboard(name, expectation_map)
    self._expectation_map = expectation_map

    # These will be set up later, in prepare_to_run(), and run_subprocess().
    self._args = None
    self._output_filename = None
    self._xvfb_output_filename = None
    self._subprocess = None
    self._user_data_dir = None

  @property
  def name(self):
    """Returns the name of this test runner."""
    return self._name

  @property
  def deadline(self):
    """Returns the deadline the test should run in."""
    return self._deadline

  @property
  def expectation_map(self):
    """Returns a map from test name to its expectation.

    Note that the test name does not contain the suite prefix.
    (i.e. "fixture_name#method_name" style).
    """
    return self._expectation_map.copy()

  @property
  def terminated(self):
    return self._terminated

  @property
  def user_data_dir(self):
    return self._user_data_dir

  def get_scoreboard(self):
    return self._scoreboard

  @property
  def expect_failure(self):
    """Returns the expected result of the whole suite."""
    return flags.FAIL in self._flags or flags.TIMEOUT in self._flags

  @property
  def bug(self):
    """Returns the bug url(s) associated with this suite."""
    return self._bug

  def is_runnable(self):
    return True

  def prepare(self, test_methods_to_run):
    """Overridden in actual implementations to do preparations on the host.

    This is invoked on the host but not invoked on a remote host (Windows, Mac,
    and Chrome OS) when --remote option is specified for run_integration_tests.
    This is a good place to prepare the files that are copied to the remote host
    for running tests on it.
    This function is invoked only once even if flaky tests are retried.
    """
    pass

  def setUp(self, test_methods_to_run):
    """Overridden in actual implementations to do pre-test setup."""
    pass

  def tearDown(self, test_methods_to_run):
    """Overridden in actual implementations to do post-test cleanup."""
    pass

  def run(self, test_methods_to_run):
    """Invoked by the framework to run one or more test methods.

    The names in test_methods_to_run will be some subset of the names returned
    by the suite_test_expectations property.

    This function should return a pair:

      (raw_test_output, test_method_results)

    raw_test_output should be a string containing the full output of running the
    suite, or something equivalent.

    test_method_results should be a dictionary mapping test method names to
    instances of test_method_results.TestMethodResult, which describes whether
    each test passed or failed, and any test specific output or error messages.

    Note that a special test method name of ALL_TESTS_DUMMY_NAME is used if the
    test runner implementation does not seem to provide any
    suite_test_expectations, and this may need to be ignored.

    This should be overridden in actual implementations as necessary to run the
    tests.
    This is invoked on Chrome OS when --remote option specified for
    run_integration_tests, so the tools that are not available on Chrome OS
    should not be used in this function (e.g. ninja, javac, dx etc.).
    """
    return "", {}

  def finalize(self, test_methods_to_run):
    """Overridden in actual implementations to do final cleanup.

    This function differs from tearDown in that tearDown can be invoked several
    times if flaky tests are retried but finalize is invoked only once at the
    end of the test.
    """
    pass

  def handle_output(self, line):
    pass

  def prepare_to_run(self, test_methods_to_run, args):
    self._args = args
    self.prepare(test_methods_to_run)

  def run_with_setup(self, test_methods_to_run, args):
    self._args = args
    try:
      self._scoreboard.start(test_methods_to_run)
      self.setUp(test_methods_to_run)
      return self.run(test_methods_to_run)
    finally:
      self.tearDown(test_methods_to_run)

  def restart(self, test_methods_to_run, args):
    self._scoreboard.restart()

  def abort(self, test_methods_to_run, args):
    self._scoreboard.abort()

  def finalize_after_run(self, test_methods_to_run, args):
    self._args = args
    self.finalize(test_methods_to_run)
    self._scoreboard.finalize()

  def apply_test_ordering(self, test_methods_to_run):
    def key_fn(name):
      matched_list = [(pattern, order)
                      for pattern, order in self._test_order.iteritems()
                      if fnmatch.fnmatch(name, pattern)]
      assert len(matched_list) < 2, (
          'Too many patterns match with the test. test_name: \'%s\', '
          'patterns: \'%s\'' % (name, matched_list))
      return ((matched_list[0][1] if matched_list else 0), name)
    return sorted(test_methods_to_run, key=key_fn)

  def get_launch_chrome_command(self, additional_args, mode=None,
                                name_override=None):
    """Returns the commandline for running suite runner with launch_chrome."""
    args = launch_chrome_util.get_launch_chrome_command()
    if mode:
      args.append(mode)
    name = name_override if name_override else self._name
    args.extend(['--crx-name-override=' + name,
                 '--noninja',
                 '--disable-sleep-on-blur'])
    if self._user_data_dir:
      args.append('--user-data-dir=' + self._user_data_dir)
    else:
      args.append('--use-temporary-data-dirs')

    # Force software GPU emulation mode when running tests under Xvfb.
    if self._args.enable_osmesa or self._args.use_xvfb:
      args.append('--enable-osmesa')

    if self._metadata:
      args.append('--additional-metadata=' + json.dumps(self._metadata))

    args.extend(self._args.launch_chrome_opts)

    deadline = self.deadline
    if self._args.min_deadline:
      deadline = max(deadline, self._args.min_deadline)
    if self._args.max_deadline:
      deadline = min(deadline, self._args.max_deadline)
    args.append('--timeout=' + str(deadline))

    return args + additional_args

  def get_system_mode_launch_chrome_command(self, name, additional_args=None):
    additional_args = build_common.as_list(additional_args)
    return self.get_launch_chrome_command(
        # We need --stderr-log=I as we will use ALOGI output from
        # AdbService. See _ADB_SERVICE_PATTERN in system_mode.py.
        ['--stderr-log=I'] + additional_args,
        mode='system',
        name_override=SYSTEM_MODE_PREFIX + name)

  def _get_subprocess_output(self):
    output = ''
    if self._output_filename:
      with open(self._output_filename, 'r') as output_file:
        output = output_file.read()
      self._output_filename = None
    return output

  def _get_xvfb_output(self):
    output = ''
    if self._xvfb_output_filename:
      with open(self._xvfb_output_filename, 'r') as output_file:
        output = output_file.read()
      self._xvfb_output_filename = None
    return output

  def get_use_xvfb(self):
    return self._args.use_xvfb

  def terminate(self):
    with self._lock:
      self._terminated = True
      if self._subprocess and self._subprocess.returncode is None:
        self._subprocess.terminate()

  def kill(self):
    with self._lock:
      if self._subprocess and self._subprocess.returncode is None:
        self._subprocess.kill()

  def write_args_if_needed(self, args):
    """Writes args to a file if it is too long and returns a new args."""
    # Do not rewrite args of the commands other than launch_chrome because
    # the commands do not necessarily support the syntax of reading arguments
    # from a file.
    if not launch_chrome_util.is_launch_chrome_command(args):
      return args
    remaining_args = launch_chrome_util.remove_leading_launch_chrome_args(args)
    args_string = '\n'.join(remaining_args)
    # Do not rewrite args to file if the argument list is short enough.
    if len(args_string) < SuiteRunnerBase.WRITE_ARGS_MIN_LENGTH:
      return args

    args_dir = os.path.join(build_common.get_build_dir(), 'integration_tests')
    file_util.makedirs_safely(args_dir)

    args_file = os.path.join(args_dir, self._name + '_args')
    with open(args_file, 'w') as f:
      f.write(args_string)
    return args[:-len(remaining_args)] + ['@' + args_file]

  # Using run-xvfb increases launching time, and is useless for command line
  # tools, e.g., javac and adb. Even though --use-xvfb is specified, do not
  # use run-xvfb for them if omit_xvfb=True.
  def run_subprocess(self, args, omit_xvfb=False, *vargs, **kwargs):
    """Runs a subprocess handling verbosity flags."""
    output_directory = SuiteRunnerBase._output_directory
    if self._args.use_xvfb and not omit_xvfb:
      self._xvfb_output_filename = os.path.abspath(
          os.path.join(output_directory, self._name + '-xvfb.log'))
      args = SuiteRunnerBase.get_xvfb_args(self._xvfb_output_filename) + args
    self._output_filename = os.path.join(output_directory, self._name)
    with open(self._output_filename, 'w') as output_file:
      with self._lock:
        if self._terminated:
          raise subprocess.CalledProcessError(1, args)
        args = self.write_args_if_needed(args)
        self._subprocess = filtered_subprocess.Popen(args, *vargs, **kwargs)
      verbose = self._args.output == 'verbose'
      handler = SuiteRunnerOutputHandler(verbose, output_file, self)
      self._subprocess.run_process_filtering_output(handler)
      returncode = self._subprocess.wait()
      # We emulate subprocess.check_call() here, as the callers expect to catch
      # a CalledProcessError when there is a problem.
      if returncode:
        output = handler.get_output()
        raise subprocess.CalledProcessError(returncode, args, output)
    output = handler.get_output()
    xvfb_output = self._get_xvfb_output()
    if xvfb_output and self._args.output == 'verbose':
      print '-' * 10 + ' XVFB output starts ' + '-' * 10
      print xvfb_output
    return output

  def launch_chrome(self, command, *args, **kwargs):
    """Runs ./launch_chrome.

    This method automatically and heuristically finds timeout caused by
    Chrome flakiness. On that failure, retries several times automatically.
    - command: a list of strings. ./launch_chrome command line including its
      arguments.
    - *args, **kwargs: these will be (eventually) passed to subprocess.Popen.
    """
    assert launch_chrome_util.is_launch_chrome_command(command), (
        'launch_chrome() is invoked with non-launch_chrome '
        'command line args: %s' % str(command))

    for trial in xrange(LAUNCH_CHROME_FLAKE_RETRY_COUNT):
      try:
        return self.run_subprocess(command, *args, **kwargs)
      except subprocess.CalledProcessError as e:
        output = e.output or ''
        # Detect Chrome flakiness error heuristically.
        # Workaround for what we suspect is a problem with Chrome failing on
        # launch a few times a day on the waterfall.  The symptom is that we
        # get 3-5 lines of raw output followed by a TIMEOUT message.
        if (output.endswith('[  TIMEOUT  ]\n') and
            output.count('\n') < _LAUNCH_CHROME_MINIMUM_LINES):
          print '@@@STEP_WARNINGS@@@'
          print '@@@STEP_TEXT@Retrying ' + self.name + ' (Chrome flake)@@@'
          continue
        raise
      except Exception:
        # Exception other than subprocess.CalledProcessError means
        # starting ./launch_chrome failed. Here, log the command line,
        # in the form to be copy-and-pasted for manual debugging.
        safe_args, unsafe_args = (
            launch_chrome_util.split_launch_chrome_args(command))
        logging.error('Launch chrome failed: %s',
                      logging_util.format_commandline(
                          args, kwargs.get('cwd'), kwargs.get('env')))
        if unsafe_args:
          logging.error(
              'NOTE: The following options were ommitted in the command line '
              'above to make it suitable for debugging use: %s',
              logging_util.format_commandline(unsafe_args))
        raise
    else:
      # Here, we hit the Chrome flakiness retry count limit,
      # so raise an exception. Note that |output| is available, because
      # except-block above should have run several times.
      # Note that we should think about the better way to return |output|.
      # For example, the output of xvfb is ignored. Also, the output of
      # former trials is also ignored. These may be useful to investigate
      # on failure.
      # TODO(crbug.com/455560): Revisit and improve logging output.
      raise subprocess.CalledProcessError(1, command, output)

  def run_subprocess_test(self, test_name, command, env=None):
    """Runs a test which runs subprocess and sets status appropriately.

    - test_name: The name of this test. This should be
      'test_fixter#test_method' style.
    - command: A list of strings. Command line to run subprocess.
    - env: Environment dict for the subprocess. Maybe None.
    """
    assert not launch_chrome_util.is_launch_chrome_command(command), (
        'For testing with ./launch_chrome, you should use launch_chrome() '
        'method instead.: %s' % str(command))

    self._scoreboard.start_test(test_name)
    try:
      raw_output = self.run_subprocess(command, env=env)
      status = test_method_result.TestMethodResult.PASS
    except subprocess.CalledProcessError as e:
      raw_output = e.output
      if self._xvfb_output_filename is not None:
        raw_output += '-' * 10 + ' XVFB output starts ' + '-' * 10
        raw_output += self._get_xvfb_output()
      status = test_method_result.TestMethodResult.FAIL
    result = test_method_result.TestMethodResult(test_name, status)
    self._scoreboard.update([result])
    return raw_output, {test_name: result}
