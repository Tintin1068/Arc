# Copyright 2014 The Chromium Authors. All rights reserved.
# Use of this source code is governed by a BSD-style license that can be
# found in the LICENSE file.

"""Defines the integration test interface to running a suite of tests."""

import fnmatch
import json
import os
import re
import subprocess
import threading
import traceback

import build_common
from util import concurrent_subprocess
from util import file_util
from util import launch_chrome_util
from util.test import scoreboard
from util.test import suite_runner_config
from util.test import suite_runner_config_flags as flags
from util.test import suite_runner_util
from util.test import test_method_result


# Number of times to retry because of Chrome startup flake.
LAUNCH_CHROME_FLAKE_RETRY_COUNT = 2
# CRX directories used by system mode should have this prefix.
SYSTEM_MODE_PREFIX = 'system_mode.'


class TimeoutError(Exception):
  """Timeout class raised in this module."""


class _NullLogger(object):
  """Logger which does nothing."""
  def write(self, text):
    pass

  def writelines(self, text_list):
    pass

  def flush(self):
    pass

  @property
  def path(self):
    return None


class SuiteRunnerLogger(object):
  """Logger for each integration test suite.

  This is just a thin wrapper for a file object for a logging purpose.
  The extra feature is "verbose" output. When "verbose" is set to True,
  this also outputs the lines to the stdout, with |suite_name| prefix for
  each line.
  """

  def __init__(self, suite_name, path, verbose):
    self._suite_name = suite_name
    self._verbose = verbose
    self._path = path

    self._file = open(path, 'w')
    self._pending = ''

  @property
  def path(self):
    return self._path

  def close(self):
    if self._pending and self._verbose:
      print '%s %s' % (self._suite_name, self._pending.rstrip())
      self._pending = ''
    return self._file.close()

  def flush(self):
    return self._file.flush()

  def write(self, text):
    self._file.write(text)
    if self._verbose:
      self._verbose_output(text)

  def writelines(self, text_list):
    self._file.writelines(text_list)
    if self._verbose:
      for text in text_list:
        self._verbose_output(text)

  def _verbose_output(self, text):
    text = self._pending + text
    if not text:
      # Nothing to be written.
      return

    lines = text.splitlines(True)  # Keepends = True
    if lines[-1].endswith('\n'):
      self._pending = ''
    else:
      self._pending = lines.pop(-1)

    for line in lines:
      print '%s %s' % (self._suite_name, line.rstrip())


class _SuiteRunnerOutputHandler(concurrent_subprocess.OutputHandler):
  """Class to handle output generated by test runners.

  When a test suite runner is run, it will use a concurrent_subprocess.Popen
  to execute the tests, using this class as an output handler.  This
  class will write all output from the test run to disk and then allow
  the runner to perform further processing.
  """
  def __init__(self, logger, runner):
    super(_SuiteRunnerOutputHandler, self).__init__()
    self._logger = logger
    self._output = []
    self._runner = runner

  def handle_stdout(self, line):
    self._handle_output(line)

  def handle_stderr(self, line):
    self._handle_output(line)

  def get_output(self):
    return ''.join(self._output)

  def _handle_output(self, line):
    self._logger.write(line)
    self._output.append(line)
    self._runner.handle_output(line)


class SuiteRunnerBase(object):
  """Base class for a test suite runner.

  A suite runner can be constructed with several keyword options:

    flags

      A combination of util.test.suite_runner_config_flags flag values that
      indicate the expected result of running the suite.

    suite_test_expectations

      A dictionary mapping fully qualified test names to an expectation flag
      value indicating if that test will pass or not.

      Example:

        suite_test_expectations = {
            'Class1': {
                'method1': PASS, # passes
                'method2': FLAKY,  # passes, but flaky
                'method3': FAIL,  # fails
                'method4': NOT_SUPPORTED  # blacklist from running
            }
        }

    deadline

        The deadline in seconds in which the test suite should run to
        completion.
    """
  DEFAULT_DEADLINE = 300

  WRITE_ARGS_MIN_LENGTH = 10000

  @staticmethod
  def get_xvfb_args(output_filename):
    command = [
        'xvfb-run', '--auto-servernum',
        # Use 24-bit color depth, as Chrome does not work with 8-bit color
        # depth, which is used in xvfb-run by default.
        '--server-args', '-screen 0 1920x1080x24'
    ]
    if output_filename:
      command.extend(['--error-file', os.path.abspath(output_filename)])
    return command

  def __init__(self, name, base_expectation_map, config=None):
    merged_config = suite_runner_config.default_run_configuration()
    if config:
      merged_config.update(config)

    self._lock = threading.Lock()
    self._name = name
    self._terminated = False
    self._deadline = merged_config.pop('deadline')
    self._bug = merged_config.pop('bug')
    self._metadata = merged_config.pop('metadata')
    self._test_order = merged_config.pop('test_order')

    default_expectation = merged_config.pop('flags')
    override_expectation_map = merged_config.pop('suite_test_expectations')
    assert not merged_config, ('Unexpected keyword arguments %s' %
                               merged_config.keys())

    expectation_map = suite_runner_util.merge_expectation_map(
        base_expectation_map, override_expectation_map, default_expectation)
    self._scoreboard = scoreboard.Scoreboard(name, expectation_map)
    self._expectation_map = expectation_map

    # These will be set up later, in prepare_to_run(), and run_subprocess().
    self._args = None
    self._logger = None
    self._subprocess = None
    self._user_data_dir = None

  @property
  def name(self):
    """Returns the name of this test runner."""
    return self._name

  @property
  def deadline(self):
    """Returns the deadline the test should run in."""
    return self._deadline

  @property
  def expectation_map(self):
    """Returns a map from test name to its expectation.

    Note that the test name does not contain the suite prefix.
    (i.e. "fixture_name#method_name" style).
    """
    return self._expectation_map.copy()

  @property
  def terminated(self):
    return self._terminated

  @property
  def user_data_dir(self):
    return self._user_data_dir

  def get_scoreboard(self):
    return self._scoreboard

  @property
  def expect_failure(self):
    """Returns the expected result of the whole suite."""
    return flags.FAIL in self._flags or flags.TIMEOUT in self._flags

  @property
  def bug(self):
    """Returns the bug url(s) associated with this suite."""
    return self._bug

  def is_runnable(self):
    return True

  def prepare(self, test_methods_to_run):
    """Overridden in actual implementations to do preparations on the host.

    This is invoked on the host but not invoked on a remote host (Windows, Mac,
    and Chrome OS) when --remote option is specified for run_integration_tests.
    This is a good place to prepare the files that are copied to the remote host
    for running tests on it.
    This function is invoked only once even if flaky tests are retried.
    """
    pass

  def setUp(self, test_methods_to_run):
    """Overridden in actual implementations to do pre-test setup."""
    pass

  def tearDown(self, test_methods_to_run):
    """Overridden in actual implementations to do post-test cleanup."""
    pass

  def run(self, test_methods_to_run):
    """Invoked by the framework to run one or more test methods.

    The names in test_methods_to_run will be some subset of the names returned
    by the suite_test_expectations property.

    Note that a special test method name of ALL_TESTS_DUMMY_NAME is used if the
    test runner implementation does not seem to provide any
    suite_test_expectations, and this may need to be ignored.

    This should be overridden in actual implementations as necessary to run the
    tests.
    This is invoked on Chrome OS when --remote option specified for
    run_integration_tests, so the tools that are not available on Chrome OS
    should not be used in this function (e.g. ninja, javac, dx etc.).
    """
    raise NotImplementedError('SuiteRunnerBase.run must be overridden.')

  def finalize(self, test_methods_to_run):
    """Overridden in actual implementations to do final cleanup.

    This function differs from tearDown in that tearDown can be invoked several
    times if flaky tests are retried but finalize is invoked only once at the
    end of the test.
    """
    pass

  def handle_output(self, line):
    pass

  @property
  def logger(self):
    return self._logger

  def prepare_to_run(self, test_methods_to_run, args):
    self._args = args
    # Sets NullLogger, so that run_subprocess can be used in prepare().
    self._logger = _NullLogger()
    try:
      self.prepare(test_methods_to_run)
    finally:
      self._logger = None

  def run_with_setup(self, test_methods_to_run, args, logger):
    self._args = args
    self._logger = logger
    try:
      try:
        self._scoreboard.start(test_methods_to_run)
        self.setUp(test_methods_to_run)
        self.run(test_methods_to_run)
      finally:
        self.tearDown(test_methods_to_run)
    except Exception:
      logger.write(traceback.format_exc())
    finally:
      logger.flush()
      self._logger = None

  def restart(self, test_methods_to_run, args):
    self._scoreboard.restart()

  def abort(self, test_methods_to_run, args):
    self._scoreboard.abort()

  def finalize_after_run(self, test_methods_to_run, args):
    self._args = args
    self.finalize(test_methods_to_run)
    self._scoreboard.finalize()

  def apply_test_ordering(self, test_methods_to_run):
    def key_fn(name):
      matched_list = [(pattern, order)
                      for pattern, order in self._test_order.iteritems()
                      if fnmatch.fnmatch(name, pattern)]
      assert len(matched_list) < 2, (
          'Too many patterns match with the test. test_name: \'%s\', '
          'patterns: \'%s\'' % (name, matched_list))
      return ((matched_list[0][1] if matched_list else 0), name)
    return sorted(test_methods_to_run, key=key_fn)

  def get_launch_chrome_command(self, additional_args, mode=None,
                                name_override=None, additional_metadata=None):
    """Returns the commandline for running suite runner with launch_chrome."""
    args = launch_chrome_util.get_launch_chrome_command()
    if mode:
      args.append(mode)
    name = name_override if name_override else self._name
    args.extend(['--crx-name-override=' + name,
                 '--noninja',
                 '--disable-sleep-on-blur'])
    if self._user_data_dir:
      args.append('--user-data-dir=' + self._user_data_dir)
    else:
      args.append('--use-temporary-data-dirs')

    # Force software GPU emulation mode when running tests under Xvfb.
    if self._args.enable_osmesa or self._args.use_xvfb:
      args.append('--enable-osmesa')

    if self._metadata or additional_metadata:
      suite_additional_metadata = self._metadata.copy() or {}
      additional_metadata = additional_metadata or {}
      assert all(key not in suite_additional_metadata
                 for key in additional_metadata), (
                     'Overlap between %s and %s' % (
                         additional_metadata, suite_additional_metadata))
      suite_additional_metadata.update(additional_metadata)
      args.append('--additional-metadata=' +
                  json.dumps(suite_additional_metadata))

    args.extend(self._args.launch_chrome_opts)

    deadline = self.deadline
    if self._args.min_deadline:
      deadline = max(deadline, self._args.min_deadline)
    if self._args.max_deadline:
      deadline = min(deadline, self._args.max_deadline)
    args.append('--timeout=' + str(deadline))

    # chrome-flakiness-retry option is incompatible with debuggers, do
    # a quick check before adding the option.
    # The flags are: --jdb, --jdb-port and --gdb.
    if not any(re.match('--(gdb(=.+)?|jdb|jdb-port(=.+)?)$', arg)
               for arg in args):
      args.append(
          '--chrome-flakiness-retry=%d' % LAUNCH_CHROME_FLAKE_RETRY_COUNT)

    return args + additional_args

  def get_system_mode_launch_chrome_command(self, name, additional_args=None,
                                            additional_metadata=None):
    additional_args = build_common.as_list(additional_args)
    return self.get_launch_chrome_command(
        # We need --stderr-log=I as we will use ALOGI output from
        # AdbService. See _ADB_SERVICE_PATTERN in system_mode.py.
        ['--stderr-log=I'] + additional_args,
        mode='system',
        name_override=SYSTEM_MODE_PREFIX + name,
        additional_metadata=additional_metadata)

  @property
  def use_xvfb(self):
    return self._args.use_xvfb

  def terminate(self):
    with self._lock:
      self._terminated = True
      if self._subprocess:
        self._subprocess.terminate()

  def kill(self):
    with self._lock:
      if self._subprocess:
        self._subprocess.kill()

  def _write_args_if_needed(self, args):
    """Writes args to a file if it is too long and returns a new args."""
    # Do not rewrite args of the commands other than launch_chrome because
    # the commands do not necessarily support the syntax of reading arguments
    # from a file.
    if not launch_chrome_util.is_launch_chrome_command(args):
      return args
    remaining_args = launch_chrome_util.remove_leading_launch_chrome_args(args)
    args_string = '\n'.join(remaining_args)
    # Do not rewrite args to file if the argument list is short enough.
    if len(args_string) < SuiteRunnerBase.WRITE_ARGS_MIN_LENGTH:
      return args

    args_dir = os.path.join(build_common.get_build_dir(), 'integration_tests')
    file_util.makedirs_safely(args_dir)

    args_file = os.path.join(args_dir, self._name + '_args')
    with open(args_file, 'w') as f:
      f.write(args_string)
    return args[:-len(remaining_args)] + ['@' + args_file]

  # Using run-xvfb increases launching time, and is useless for command line
  # tools, e.g., javac and adb. Even though --use-xvfb is specified, do not
  # use run-xvfb for them if omit_xvfb=True.
  def run_subprocess(self, args, omit_xvfb=False, *vargs, **kwargs):
    """Runs a subprocess handling verbosity flags."""
    use_xvfb = self._args.use_xvfb and not omit_xvfb

    # Set up xvfb command and its log file.
    if use_xvfb:
      # Note: this may override the old log, but it should be ok,
      # because older logs are already copied to the logger.
      if self._logger.path:
        xvfb_output_filepath = self._logger.path + '-xvfb.log'
      else:
        xvfb_output_filepath = None
      # Prepend the command line.
      args = SuiteRunnerBase.get_xvfb_args(xvfb_output_filepath) + args
    else:
      # No XVFB is needed.
      xvfb_output_filepath = None

    # Run the process.
    self._logger.writelines([
        '-------------------- run_subprocess: %s\n' % args[0],
        ' '.join(args),
        '\n--------------------\n'])
    args = self._write_args_if_needed(args)
    with self._lock:
      if self._terminated:
        # Terminate is already called.
        self._logger.write(
            '-------------------- %s: done: %d\n' % (args[0], 1))
        raise subprocess.CalledProcessError(1, args)
      self._subprocess = concurrent_subprocess.Popen(args, *vargs, **kwargs)
    handler = _SuiteRunnerOutputHandler(self._logger, self)
    returncode = self._subprocess.handle_output(handler)
    self._logger.write(
        '-------------------- %s: done: %d\n' % (args[0], returncode))

    # Output XVFB's log, if necessary.
    if xvfb_output_filepath:
      with open(xvfb_output_filepath) as f:
        xvfb_output = f.read()
      self._logger.writelines([
          '---------- XVFB output ----------\n',
          xvfb_output,
          '---------------------------------\n'])
    self._logger.flush()

    # We emulate subprocess.check_call() here, as the callers expect to catch
    # a CalledProcessError when there is a problem.
    output = handler.get_output()
    if returncode:
      raise subprocess.CalledProcessError(returncode, args, output)
    return output

  def run_subprocess_test(self, test_name, command, env=None):
    """Runs a test which runs subprocess and sets status appropriately.

    - test_name: The name of this test. This should be
      'test_fixter#test_method' style.
    - command: A list of strings. Command line to run subprocess.
    - env: Environment dict for the subprocess. Maybe None.
    """
    assert not launch_chrome_util.is_launch_chrome_command(command), (
        'For testing with ./launch_chrome, you should use run_subprocess() '
        'method instead.: %s' % str(command))

    self._scoreboard.start_test(test_name)
    try:
      self.run_subprocess(command, env=env)
      status = test_method_result.TestMethodResult.PASS
    except subprocess.CalledProcessError:
      status = test_method_result.TestMethodResult.FAIL
    result = test_method_result.TestMethodResult(test_name, status)
    self._scoreboard.update([result])
